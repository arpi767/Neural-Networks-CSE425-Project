# -*- coding: utf-8 -*-
"""425_Easy Tasks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11k46BCF7F4gJS08oo-BYRJCCXZe0szFm
"""

!pip -q install datasets librosa soundfile umap-learn scikit-learn torch torchcodec
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F

import librosa

from datasets import load_dataset, Audio

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score
from sklearn.manifold import TSNE

try:
    import umap
    HAS_UMAP = True
except Exception:
    HAS_UMAP = False

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", DEVICE, "| UMAP:", HAS_UMAP)

TARGET_SR = 22050

ds = load_dataset("jamendolyrics/jamendolyrics", split="test")
print(ds)
print("Columns:", ds.column_names)

ds = ds.cast_column("audio", Audio(sampling_rate=TARGET_SR))
print("Audio cast done.")

def get_audio(example, target_sr=22050):
    a = example["audio"]

    try:
        y = a["array"]
        sr = a["sampling_rate"]
        return y.astype(np.float32), int(sr)
    except Exception:
        pass

    try:
        if isinstance(a, dict) and "path" in a:
            path = a["path"]
        else:
            path = getattr(a, "path", None)
        if path is None:
            path = example["audio"]["path"]
        y, sr = librosa.load(path, sr=target_sr, mono=True)
        return y.astype(np.float32), int(sr)
    except Exception as e:
        raise RuntimeError(f"Audio extraction failed: {e}")

y0, sr0 = get_audio(ds[0], TARGET_SR)
print("Audio OK:", y0.shape, "sr:", sr0)

def norm_lang(x):
    if x is None:
        return ""
    return str(x).strip().lower()

langs = [norm_lang(ds[i].get("language", "")) for i in range(len(ds))]
uniq_langs = sorted(set(langs))
print("Unique languages in split:", uniq_langs)

EN_SET = {"en", "english"}
BN_SET = {"bn", "bangla", "bengali", "বাংলা"}

idx_keep = []
for i in range(len(ds)):
    l = norm_lang(ds[i].get("language", ""))
    if (l in EN_SET) or (l in BN_SET):
        idx_keep.append(i)

print("Total rows:", len(ds), "| EN/BN rows:", len(idx_keep))

if len(idx_keep) < 10:
    print("[WARN] Too few EN/BN samples found. Using the full split instead.")
    idx_keep = list(range(len(ds)))

ds_sub = ds.select(idx_keep)

langs_sub = [norm_lang(ds_sub[i].get("language","")) for i in range(len(ds_sub))]
print(pd.Series(langs_sub).value_counts())
print("Subset rows:", len(ds_sub))

CLIP_SECONDS = 20.0
N_MFCC = 20

def fix_length_audio(y, sr, clip_seconds):
    target_len = int(sr * clip_seconds)
    if len(y) < target_len:
        y = np.pad(y, (0, target_len - len(y)))
    else:
        y = y[:target_len]
    return y

def mfcc_features(y, sr, n_mfcc=20):
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)  #(n_mfcc, T)
    mu = mfcc.mean(axis=1)
    sd = mfcc.std(axis=1)
    return np.concatenate([mu, sd]).astype(np.float32)      #(2*n_mfcc,)

X = []
y_lang = []
for i in range(len(ds_sub)):
    ex = ds_sub[i]
    y, sr = get_audio(ex, TARGET_SR)
    y = fix_length_audio(y, sr, CLIP_SECONDS)
    X.append(mfcc_features(y, sr, N_MFCC))
    y_lang.append(norm_lang(ex.get("language","")))

X = np.stack(X, axis=0)  #(N, 40)
print("Feature matrix:", X.shape)
print("Languages in subset:", pd.Series(y_lang).value_counts())

scaler = StandardScaler()
Xn = scaler.fit_transform(X)
print("Standardized:", Xn.shape)

class VAE(nn.Module):
    def __init__(self, input_dim, latent_dim=8, hidden_dim=64):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)

        self.fc2 = nn.Linear(latent_dim, hidden_dim)
        self.fc_out = nn.Linear(hidden_dim, input_dim)

    def encode(self, x):
        h = F.relu(self.fc1(x))
        return self.fc_mu(h), self.fc_logvar(h)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        h = F.relu(self.fc2(z))
        return self.fc_out(h)

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        x_hat = self.decode(z)
        return x_hat, mu, logvar

def vae_loss(x, x_hat, mu, logvar, beta=1.0):
    recon = F.mse_loss(x_hat, x, reduction="mean")
    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
    return recon + beta * kl, recon.detach(), kl.detach()

LATENT_DIM = 8
HIDDEN_DIM = 64
EPOCHS = 80
BATCH_SIZE = 16
LR = 1e-3
BETA = 1.0

vae = VAE(input_dim=Xn.shape[1], latent_dim=LATENT_DIM, hidden_dim=HIDDEN_DIM).to(DEVICE)
opt = torch.optim.Adam(vae.parameters(), lr=LR)

Xt = torch.from_numpy(Xn.astype(np.float32))

hist = {"loss": [], "recon": [], "kl": []}

vae.train()
for epoch in range(1, EPOCHS + 1):
    perm = torch.randperm(Xt.size(0))
    losses, recons, kls = [], [], []

    for i in range(0, Xt.size(0), BATCH_SIZE):
        idx = perm[i:i+BATCH_SIZE]
        xb = Xt[idx].to(DEVICE)

        opt.zero_grad()
        x_hat, mu, logvar = vae(xb)
        loss, recon, kl = vae_loss(xb, x_hat, mu, logvar, beta=BETA)
        loss.backward()
        opt.step()

        losses.append(loss.item())
        recons.append(recon.item())
        kls.append(kl.item())

    hist["loss"].append(float(np.mean(losses)))
    hist["recon"].append(float(np.mean(recons)))
    hist["kl"].append(float(np.mean(kls)))

    if epoch == 1 or epoch % 10 == 0:
        print(f"Epoch {epoch:03d} | loss={hist['loss'][-1]:.4f} recon={hist['recon'][-1]:.4f} kl={hist['kl'][-1]:.4f}")

plt.figure()
plt.plot(hist["loss"], label="total")
plt.plot(hist["recon"], label="recon")
plt.plot(hist["kl"], label="kl")
plt.legend()
plt.title("VAE training curves")
plt.xlabel("epoch"); plt.ylabel("loss")
plt.show()

vae.eval()
with torch.no_grad():
    mu, logvar = vae.encode(Xt.to(DEVICE))
    Z_vae = mu.cpu().numpy()

print("Z_vae:", Z_vae.shape)

PCA_DIM = min(8, Xn.shape[1])
pca = PCA(n_components=PCA_DIM, random_state=SEED)
Z_pca = pca.fit_transform(Xn)

print("Z_pca:", Z_pca.shape)

def kmeans_metrics(Z, k_values):
    rows = []
    for k in k_values:
        labels = KMeans(n_clusters=k, n_init=10, random_state=SEED).fit_predict(Z)
        sil = silhouette_score(Z, labels) if len(set(labels)) > 1 else np.nan
        ch = calinski_harabasz_score(Z, labels) if len(set(labels)) > 1 else np.nan
        rows.append({"k": k, "silhouette": sil, "calinski_harabasz": ch})
    return pd.DataFrame(rows)

k_values = list(range(2, 11))

df_vae = kmeans_metrics(Z_vae, k_values)
df_vae["method"] = "VAE_latent + KMeans"

df_pca = kmeans_metrics(Z_pca, k_values)
df_pca["method"] = "PCA + KMeans"

df_all = pd.concat([df_vae, df_pca], ignore_index=True)
display(df_all)

def plot_metric(df, metric, title):
    plt.figure(figsize=(7,4))
    for m, g in df.groupby("method"):
        g = g.sort_values("k")
        plt.plot(g["k"], g[metric], marker="o", label=m)
    plt.title(title)
    plt.xlabel("k")
    plt.ylabel(metric)
    plt.legend()
    plt.show()

plot_metric(df_all, "silhouette", "Silhouette vs k (VAE vs PCA)")
plot_metric(df_all, "calinski_harabasz", "Calinski–Harabasz vs k (VAE vs PCA)")

best_k_vae = int(df_vae.sort_values("silhouette", ascending=False).iloc[0]["k"])
best_k_pca = int(df_pca.sort_values("silhouette", ascending=False).iloc[0]["k"])

labels_vae = KMeans(n_clusters=best_k_vae, n_init=10, random_state=SEED).fit_predict(Z_vae)
labels_pca = KMeans(n_clusters=best_k_pca, n_init=10, random_state=SEED).fit_predict(Z_pca)

print("Best k (VAE):", best_k_vae, "| Best k (PCA):", best_k_pca)

def tsne_2d(Z):
    n = Z.shape[0]
    perplex = min(20, max(5, (n - 1)//3))
    return TSNE(n_components=2, random_state=SEED, perplexity=perplex, init="pca", learning_rate="auto").fit_transform(Z)

E_vae_tsne = tsne_2d(Z_vae)
E_pca_tsne = tsne_2d(Z_pca)

def scatter(E, labels, title):
    plt.figure(figsize=(6,5))
    plt.scatter(E[:,0], E[:,1], c=labels, s=30)
    plt.title(title)
    plt.xlabel("d1"); plt.ylabel("d2")
    plt.show()

scatter(E_vae_tsne, labels_vae, f"VAE latent — t-SNE — KMeans(k={best_k_vae})")
scatter(E_pca_tsne, labels_pca, f"PCA — t-SNE — KMeans(k={best_k_pca})")

if not HAS_UMAP:
    print("UMAP not available. Install umap-learn or re-run Cell 1.")
else:
    reducer = umap.UMAP(n_components=2, random_state=SEED, n_neighbors=10, min_dist=0.15)

    E_vae_umap = reducer.fit_transform(Z_vae)
    E_pca_umap = reducer.fit_transform(Z_pca)

    scatter(E_vae_umap, labels_vae, f"VAE latent — UMAP — KMeans(k={best_k_vae})")
    scatter(E_pca_umap, labels_pca, f"PCA — UMAP — KMeans(k={best_k_pca})")

uniq = sorted(set(y_lang))
lang_to_id = {l:i for i,l in enumerate(uniq)}
y_lang_id = np.array([lang_to_id[l] for l in y_lang], dtype=int)

scatter(E_vae_tsne, y_lang_id, "VAE latent t-SNE — colored by LANGUAGE")
if HAS_UMAP:
    scatter(E_vae_umap, y_lang_id, "VAE latent UMAP — colored by LANGUAGE")

df_all.to_csv("easy_task_jamendolyrics_results.csv", index=False)
print("Saved: easy_task_jamendolyrics_results.csv")
from google.colab import files
files.download("easy_task_jamendolyrics_results.csv")