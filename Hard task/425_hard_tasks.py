# -*- coding: utf-8 -*-
"""425_Hard Tasks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FgAoEhLKKHyxeHhWJJSHYVR6G-4Fl4Qp
"""

!pip -q install datasets transformers accelerate librosa soundfile umap-learn torchcodec

import os, random
from dataclasses import dataclass
from typing import Optional, List, Tuple, Dict

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

import librosa

from datasets import load_dataset, Audio

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import (
    silhouette_score,
    adjusted_rand_score,
    normalized_mutual_info_score
)
from sklearn.manifold import TSNE

try:
    import umap
    HAS_UMAP = True
except Exception:
    HAS_UMAP = False

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", DEVICE, "| UMAP:", HAS_UMAP)

TARGET_SR = 22050

ds = load_dataset("jamendolyrics/jamendolyrics", split="test")
print(ds)
print("Columns:", ds.column_names)

ds = ds.cast_column("audio", Audio(sampling_rate=TARGET_SR))
print("Audio cast done.")

def get_audio(example, target_sr=22050) -> Tuple[np.ndarray, int]:
    a = example["audio"]
    try:
        y = a["array"]
        sr = a["sampling_rate"]
        return y.astype(np.float32), int(sr)
    except Exception:
        pass

    try:
        if isinstance(a, dict) and "path" in a:
            path = a["path"]
        else:
            path = getattr(a, "path", None)
        if path is None:
            path = example["audio"]["path"]
        y, sr = librosa.load(path, sr=target_sr, mono=True)
        return y.astype(np.float32), int(sr)
    except Exception as e:
        raise RuntimeError(f"Audio extraction failed: {e}")

y0, sr0 = get_audio(ds[0], TARGET_SR)
print("Audio OK:", y0.shape, "sr:", sr0)
print("Lyrics preview:", (ds[0].get("text","") or "")[:120], "...")

@dataclass
class AudioConfig:
    n_mels: int = 64
    n_fft: int = 1024
    hop_length: int = 256
    clip_seconds: float = 20.0     # shorter = faster
    max_frames: int = 384          # smaller = faster

@dataclass
class TrainConfig:
    latent_dim: int = 32
    batch_size: int = 8
    lr: float = 1e-3
    epochs: int = 35
    beta: float = 1.0              #CVAE

acfg = AudioConfig()
tcfg = TrainConfig()
acfg, tcfg

def fix_length_audio(y: np.ndarray, sr: int, clip_seconds: float) -> np.ndarray:
    target_len = int(sr * clip_seconds)
    if len(y) < target_len:
        y = np.pad(y, (0, target_len - len(y)))
    else:
        y = y[:target_len]
    return y

def logmel_spectrogram(y: np.ndarray, sr: int, acfg: AudioConfig) -> np.ndarray:
    y = fix_length_audio(y, sr, acfg.clip_seconds)

    S = librosa.feature.melspectrogram(
        y=y, sr=sr,
        n_fft=acfg.n_fft,
        hop_length=acfg.hop_length,
        n_mels=acfg.n_mels,
        power=2.0
    )  #(n_mels, T)

    S = np.log1p(S).astype(np.float32)

    #pad/crop frames
    T = S.shape[1]
    if T < acfg.max_frames:
        S = np.pad(S, ((0,0),(0, acfg.max_frames - T)))
    else:
        S = S[:, :acfg.max_frames]
    return S  #(n_mels, max_frames)

audio_specs = []
texts = []
genres_raw = []
langs_raw = []

for i in range(len(ds)):
    y, sr = get_audio(ds[i], TARGET_SR)
    S = logmel_spectrogram(y, sr, acfg)
    audio_specs.append(S)

    texts.append(ds[i].get("text", "") or "")
    genres_raw.append(ds[i].get("genre", None))
    langs_raw.append(ds[i].get("language", None))

X_audio = np.stack(audio_specs, axis=0)          #(N, n_mels, T)
X_audio = X_audio[:, None, :, :]                 #(N, 1, n_mels, T)

print("Audio tensor:", X_audio.shape)
print("Sample genre/lang:", genres_raw[0], langs_raw[0])

mean = X_audio.mean()
std = X_audio.std() + 1e-8
X_audio_n = ((X_audio - mean) / std).astype(np.float32)
print("Normalized:", X_audio_n.shape, X_audio_n.dtype)

def make_ids_with_unknown(values: List[Optional[str]]) -> Tuple[Dict[str,int], np.ndarray]:
    vals = [("unknown" if v is None else str(v)) for v in values]
    uniq = sorted(set(vals))
    m = {u:i for i,u in enumerate(uniq)}
    arr = np.array([m[v] for v in vals], dtype=int)
    return m, arr

genre_map, y_genre_cond = make_ids_with_unknown(genres_raw)
lang_map,  y_lang_cond  = make_ids_with_unknown(langs_raw)

G_onehot = np.eye(len(genre_map), dtype=np.float32)[y_genre_cond]
L_onehot = np.eye(len(lang_map),  dtype=np.float32)[y_lang_cond]
C = np.concatenate([G_onehot, L_onehot], axis=1).astype(np.float32)
cond_dim = C.shape[1]

print("Condition dim:", cond_dim, "| genres:", len(genre_map), "| langs:", len(lang_map))
print("C shape:", C.shape)

def make_eval_ids(values: List[Optional[str]]) -> Optional[np.ndarray]:
    if values is None or all(v is None for v in values):
        return None
    if any(v is None for v in values):
        print("[INFO] Some labels missing -> label-based metrics will be skipped.")
        return None
    uniq = sorted(set(values))
    m = {u:i for i,u in enumerate(uniq)}
    return np.array([m[v] for v in values], dtype=int)

y_genre_eval = make_eval_ids(genres_raw)
y_lang_eval  = make_eval_ids(langs_raw)

print("Genre metrics available?", y_genre_eval is not None)
print("Language plots available?", y_lang_eval is not None)

from transformers import AutoTokenizer, AutoModel

LYRICS_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
tokenizer = AutoTokenizer.from_pretrained(LYRICS_MODEL)
text_model = AutoModel.from_pretrained(LYRICS_MODEL).to(DEVICE)
text_model.eval()

@torch.no_grad()
def mean_pooling(last_hidden_state, attention_mask):
    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
    summed = torch.sum(last_hidden_state * mask, dim=1)
    counts = torch.clamp(mask.sum(dim=1), min=1e-9)
    return summed / counts

@torch.no_grad()
def embed_texts(texts: List[str], batch_size: int = 16) -> np.ndarray:
    embs = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        enc = tokenizer(batch, padding=True, truncation=True, max_length=256, return_tensors="pt").to(DEVICE)
        out = text_model(**enc)
        sent = mean_pooling(out.last_hidden_state, enc["attention_mask"])
        embs.append(sent.cpu().numpy())
    return np.vstack(embs)

Z_lyrics = embed_texts(texts, batch_size=16)
print("Lyrics embeddings:", Z_lyrics.shape)

class AudioCondDataset(Dataset):
    def __init__(self, X_audio: np.ndarray, C: np.ndarray):
        self.X = torch.from_numpy(X_audio)   # (N,1,n_mels,T)
        self.C = torch.from_numpy(C)         # (N,cond_dim)
    def __len__(self):
        return self.X.shape[0]
    def __getitem__(self, idx):
        return self.X[idx], self.C[idx]

class ConvCVAE(nn.Module):
    def __init__(self, latent_dim: int, cond_dim: int, n_mels: int, max_frames: int):
        super().__init__()
        self.latent_dim = latent_dim
        self.cond_dim = cond_dim

        self.enc_conv = nn.Sequential(
            nn.Conv2d(1, 32, 4, stride=2, padding=1), nn.ReLU(),
            nn.Conv2d(32, 64, 4, stride=2, padding=1), nn.ReLU(),
            nn.Conv2d(64, 128, 4, stride=2, padding=1), nn.ReLU(),
            )

        with torch.no_grad():
            dummy = torch.zeros(1, 1, n_mels, max_frames)
            h = self.enc_conv(dummy)
            self.enc_shape = h.shape[1:]  # (C,H,W)
            flat_dim = int(np.prod(self.enc_shape))

        self.enc_fc = nn.Linear(flat_dim + cond_dim, 256)
        self.fc_mu = nn.Linear(256, latent_dim)
        self.fc_logvar = nn.Linear(256, latent_dim)

        self.dec_fc1 = nn.Linear(latent_dim + cond_dim, 256)
        self.dec_fc2 = nn.Linear(256, flat_dim)

        self.dec_deconv = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1), nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1), nn.ReLU(),
            nn.ConvTranspose2d(32, 1, 4, stride=2, padding=1)
        )

    def encode(self, x, c):
        h = self.enc_conv(x).view(x.size(0), -1)
        hc = torch.cat([h, c], dim=1)
        h2 = F.relu(self.enc_fc(hc))
        mu = self.fc_mu(h2)
        logvar = self.fc_logvar(h2)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z, c):
        zc = torch.cat([z, c], dim=1)
        h = F.relu(self.dec_fc1(zc))
        h = self.dec_fc2(h).view(z.size(0), *self.enc_shape)
        return self.dec_deconv(h)

    def forward(self, x, c):
        mu, logvar = self.encode(x, c)
        z = self.reparameterize(mu, logvar)
        x_hat = self.decode(z, c)
        return x_hat, mu, logvar

def cvae_loss(x, x_hat, mu, logvar, beta: float = 1.0):
    recon = F.mse_loss(x_hat, x, reduction="mean")
    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
    return recon + beta * kl, recon.detach(), kl.detach()

cond_ds = AudioCondDataset(X_audio_n, C)
cond_loader = DataLoader(cond_ds, batch_size=tcfg.batch_size, shuffle=True, drop_last=False)

cvae = ConvCVAE(tcfg.latent_dim, cond_dim, acfg.n_mels, acfg.max_frames).to(DEVICE)
opt = torch.optim.Adam(cvae.parameters(), lr=tcfg.lr)

hist = {"loss": [], "recon": [], "kl": []}

cvae.train()
for epoch in range(1, tcfg.epochs + 1):
    losses, recons, kls = [], [], []
    for xb, cb in cond_loader:
        xb = xb.to(DEVICE)
        cb = cb.to(DEVICE)

        opt.zero_grad()
        x_hat, mu, logvar = cvae(xb, cb)
        loss, recon, kl = cvae_loss(xb, x_hat, mu, logvar, beta=tcfg.beta)
        loss.backward()
        opt.step()

        losses.append(loss.item())
        recons.append(recon.item())
        kls.append(kl.item())

    hist["loss"].append(float(np.mean(losses)))
    hist["recon"].append(float(np.mean(recons)))
    hist["kl"].append(float(np.mean(kls)))

    if epoch == 1 or epoch % 5 == 0:
        print(f"Epoch {epoch:03d} | loss={hist['loss'][-1]:.4f} recon={hist['recon'][-1]:.4f} kl={hist['kl'][-1]:.4f}")

plt.figure()
plt.plot(hist["loss"], label="total")
plt.plot(hist["recon"], label="recon")
plt.plot(hist["kl"], label="kl")
plt.legend()
plt.title("CVAE training curves")
plt.xlabel("epoch"); plt.ylabel("loss")
plt.show()

cvae.eval()
with torch.no_grad():
    X_t = torch.from_numpy(X_audio_n).to(DEVICE)
    C_t = torch.from_numpy(C).to(DEVICE)
    mu, logvar = cvae.encode(X_t, C_t)
    Z_audio_cvae = mu.cpu().numpy()

print("Z_audio_cvae:", Z_audio_cvae.shape)

# Reduce lyrics
LYRICS_DIM = 32
Z_lyrics_r = PCA(n_components=min(LYRICS_DIM, Z_lyrics.shape[1]), random_state=SEED).fit_transform(Z_lyrics)

Za = StandardScaler().fit_transform(Z_audio_cvae)
Zl = StandardScaler().fit_transform(Z_lyrics_r)
Zc = StandardScaler().fit_transform(C)

Z_multi = np.concatenate([Za, Zl, Zc], axis=1)
print("Z_multi:", Z_multi.shape)

def purity_score(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    if y_true is None:
        return np.nan
    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)
    total = 0
    for c in np.unique(y_pred):
        idx = np.where(y_pred == c)[0]
        if len(idx) == 0:
            continue
        counts = np.bincount(y_true[idx])
        total += counts.max()
    return total / len(y_true)

def evaluate_clustering(X_feat: np.ndarray, labels: np.ndarray, y_true: Optional[np.ndarray]) -> Dict[str, float]:
    if len(set(labels)) < 2:
        sil = np.nan
    else:
        sil = float(silhouette_score(X_feat, labels))

    if y_true is None:
        nmi = np.nan
        ari = np.nan
        pur = np.nan
    else:
        nmi = float(normalized_mutual_info_score(y_true, labels))
        ari = float(adjusted_rand_score(y_true, labels))
        pur = float(purity_score(y_true, labels))

    return {"silhouette": sil, "NMI": nmi, "ARI": ari, "purity": pur}

k_list = list(range(2, 11))
results = []

def kmeans_sweep(X_feat: np.ndarray, tag: str, y_true: Optional[np.ndarray]):
    for k in k_list:
        lab = KMeans(n_clusters=k, n_init=10, random_state=SEED).fit_predict(X_feat)
        met = evaluate_clustering(X_feat, lab, y_true)
        results.append({"method": tag, "k": k, **met})

# Main: CVAE multimodal
kmeans_sweep(Z_multi, "CVAE(audio|genre+lang)+Lyrics+Cond → KMeans", y_genre_eval)
print("Done main sweep.")

X_flat = X_audio_n.reshape(X_audio_n.shape[0], -1)

PCA_DIM = 64
Z_pca = PCA(n_components=min(PCA_DIM, X_flat.shape[1]), random_state=SEED).fit_transform(X_flat)
Z_pca = StandardScaler().fit_transform(Z_pca)

kmeans_sweep(Z_pca, "PCA(flat_spectrogram) → KMeans", y_genre_eval)
print("Done PCA baseline.")

class ConvAutoencoder(nn.Module):
    def __init__(self, bottleneck_dim: int, n_mels: int, max_frames: int):
        super().__init__()
        self.enc = nn.Sequential(
            nn.Conv2d(1, 32, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(32, 64, 4, 2, 1), nn.ReLU(),
            nn.Conv2d(64, 128, 4, 2, 1), nn.ReLU(),
        )
        with torch.no_grad():
            dummy = torch.zeros(1,1,n_mels,max_frames)
            h = self.enc(dummy)
            self.enc_shape = h.shape[1:]
            flat_dim = int(np.prod(self.enc_shape))

        self.fc = nn.Linear(flat_dim, bottleneck_dim)
        self.fc_back = nn.Linear(bottleneck_dim, flat_dim)

        self.dec = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.ReLU(),
            nn.ConvTranspose2d(32, 1, 4, 2, 1)
        )

    def encode(self, x):
        h = self.enc(x).view(x.size(0), -1)
        return self.fc(h)

    def decode(self, z):
        h = self.fc_back(z).view(z.size(0), *self.enc_shape)
        return self.dec(h)

    def forward(self, x):
        z = self.encode(x)
        x_hat = self.decode(z)
        return x_hat, z

AE_DIM = 32
ae = ConvAutoencoder(AE_DIM, acfg.n_mels, acfg.max_frames).to(DEVICE)
opt_ae = torch.optim.Adam(ae.parameters(), lr=1e-3)

loader_audio_only = DataLoader(torch.from_numpy(X_audio_n), batch_size=tcfg.batch_size, shuffle=True)

ae_hist = []
ae.train()
for epoch in range(1, 21):
    losses = []
    for xb in loader_audio_only:
        xb = xb.to(DEVICE)
        opt_ae.zero_grad()
        x_hat, z = ae(xb)
        loss = F.mse_loss(x_hat, xb, reduction="mean")
        loss.backward()
        opt_ae.step()
        losses.append(loss.item())
    ae_hist.append(float(np.mean(losses)))
    if epoch == 1 or epoch % 5 == 0:
        print(f"AE Epoch {epoch:02d} | loss={ae_hist[-1]:.4f}")

plt.figure()
plt.plot(ae_hist)
plt.title("Autoencoder training loss")
plt.xlabel("epoch"); plt.ylabel("mse")
plt.show()

ae.eval()
with torch.no_grad():
    X_t = torch.from_numpy(X_audio_n).to(DEVICE)
    Z_ae = ae.encode(X_t).cpu().numpy()

Z_ae = StandardScaler().fit_transform(Z_ae)
kmeans_sweep(Z_ae, "Autoencoder(audio) → KMeans", y_genre_eval)
print("Done AE baseline.")

def mfcc_stats(y: np.ndarray, sr: int, n_mfcc: int = 40) -> np.ndarray:
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)
    mu = mfcc.mean(axis=1)
    sd = mfcc.std(axis=1)
    return np.concatenate([mu, sd]).astype(np.float32)

X_spec = []
for i in range(len(ds)):
    y, sr = get_audio(ds[i], TARGET_SR)
    y = fix_length_audio(y, sr, acfg.clip_seconds)
    X_spec.append(mfcc_stats(y, sr, n_mfcc=40))

X_spec = StandardScaler().fit_transform(np.stack(X_spec, axis=0))
kmeans_sweep(X_spec, "DirectSpectral(MFCC mean+std) → KMeans", y_genre_eval)
print("Done direct spectral baseline.")

df_res = pd.DataFrame(results)
display(df_res.sort_values(["method","silhouette"], ascending=[True,False]).head(25))

def best_row_per_method(df: pd.DataFrame) -> pd.DataFrame:
    out = []
    for m, g in df.groupby("method"):
        g2 = g.copy()
        g2["sil_fill"] = g2["silhouette"].fillna(-np.inf)
        best = g2.sort_values("sil_fill", ascending=False).iloc[0].drop(labels=["sil_fill"])
        out.append(best)
    return pd.DataFrame(out).sort_values("silhouette", ascending=False)

best = best_row_per_method(df_res)
display(best)

def plot_curve(df, metric, title):
    plt.figure(figsize=(7,4))
    for m, g in df.groupby("method"):
        g = g.sort_values("k")
        plt.plot(g["k"], g[metric], marker="o", label=m)
    plt.title(title)
    plt.xlabel("k")
    plt.ylabel(metric)
    plt.legend()
    plt.show()

plot_curve(df_res, "silhouette", "Silhouette vs k")
plot_curve(df_res, "NMI", "NMI vs k (if genre labels available)")
plot_curve(df_res, "ARI", "ARI vs k (if genre labels available)")
plot_curve(df_res, "purity", "Purity vs k (if genre labels available)")

def embed_tsne(X_feat):
    n = X_feat.shape[0]
    perplex = min(20, max(5, (n - 1)//3))
    return TSNE(n_components=2, random_state=SEED, perplexity=perplex, init="pca", learning_rate="auto").fit_transform(X_feat)

def embed_umap(X_feat):
    if not HAS_UMAP:
        return None
    return umap.UMAP(n_components=2, random_state=SEED, n_neighbors=10, min_dist=0.15).fit_transform(X_feat)

def plot_embed(E, labels, title):
    plt.figure(figsize=(6,5))
    plt.scatter(E[:,0], E[:,1], c=labels, s=30)
    plt.title(title); plt.xlabel("d1"); plt.ylabel("d2")
    plt.show()

main_rows = df_res[df_res["method"].str.startswith("CVAE(audio|genre+lang)")]
k_best = int(main_rows.sort_values("silhouette", ascending=False).iloc[0]["k"])
lab_best = KMeans(n_clusters=k_best, n_init=10, random_state=SEED).fit_predict(Z_multi)

E_ts = embed_tsne(Z_multi)
plot_embed(E_ts, lab_best, f"Z_multi — t-SNE — KMeans(k={k_best})")

E_um = embed_umap(Z_multi)
if E_um is not None:
    plot_embed(E_um, lab_best, f"Z_multi — UMAP — KMeans(k={k_best})")


if y_lang_eval is not None:
    plot_embed(E_ts, y_lang_eval, "t-SNE — colored by LANGUAGE (interpretation)")
    if E_um is not None:
        plot_embed(E_um, y_lang_eval, "UMAP — colored by LANGUAGE (interpretation)")
if y_genre_eval is not None:
    plot_embed(E_ts, y_genre_eval, "t-SNE — colored by GENRE (interpretation)")
    if E_um is not None:
        plot_embed(E_um, y_genre_eval, "UMAP — colored by GENRE (interpretation)")

def cluster_distribution(labels: np.ndarray, y: Optional[np.ndarray], name: str):
    if y is None:
        print(f"[INFO] No {name} labels available.")
        return
    df = pd.DataFrame({"cluster": labels, name: y})
    tab = pd.crosstab(df["cluster"], df[name], normalize="index")
    display(tab)

    tab.plot(kind="bar", stacked=True, figsize=(9,4))
    plt.title(f"Cluster distribution over {name}")
    plt.xlabel("cluster")
    plt.ylabel("fraction")
    plt.tight_layout()
    plt.show()

cluster_distribution(lab_best, y_lang_eval, "language_id")
cluster_distribution(lab_best, y_genre_eval, "genre_id")

cvae.eval()
with torch.no_grad():
    X_t = torch.from_numpy(X_audio_n).to(DEVICE)
    C_t = torch.from_numpy(C).to(DEVICE)
    X_rec = cvae(X_t, C_t)[0].cpu().numpy()

def show_recon(idx_list):
    for idx in idx_list:
        orig = X_audio_n[idx, 0]
        rec  = X_rec[idx, 0]

        plt.figure(figsize=(10,3))
        plt.subplot(1,2,1)
        plt.imshow(orig, aspect="auto", origin="lower")
        plt.title(f"Original log-mel (idx={idx})")
        plt.xlabel("time"); plt.ylabel("mel")

        plt.subplot(1,2,2)
        plt.imshow(rec, aspect="auto", origin="lower")
        plt.title("Reconstruction (CVAE)")
        plt.xlabel("time"); plt.ylabel("mel")

        plt.tight_layout()
        plt.show()

show_recon([0, 1, 2, 3])

cvae.eval()
with torch.no_grad():
    c0 = torch.from_numpy(C[0:1]).to(DEVICE).repeat(4, 1)
    z = torch.randn(4, tcfg.latent_dim).to(DEVICE)
    x_gen = cvae.decode(z, c0).cpu().numpy()

for i in range(4):
    plt.figure(figsize=(6,3))
    plt.imshow(x_gen[i,0], aspect="auto", origin="lower")
    plt.title("Generated spectrogram from random z (conditioned)")
    plt.xlabel("time"); plt.ylabel("mel")
    plt.tight_layout()
    plt.show()

df_res.to_csv("hard_task_jamendolyrics_results.csv", index=False)
print("Saved: hard_task_jamendolyrics_results.csv")
from google.colab import files
files.download("hard_task_jamendolyrics_results.csv")