# -*- coding: utf-8 -*-
"""425_Medium Tasks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MaFiYjYLtJuUYMLZpLrMlpQATJxKQ_Ys
"""

!pip -q install datasets transformers accelerate librosa soundfile umap-learn torchcodec

import os, random, math
from dataclasses import dataclass
from typing import Optional, Dict, Tuple, List

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

import librosa

from datasets import load_dataset, Audio

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import silhouette_score, davies_bouldin_score, adjusted_rand_score
from sklearn.manifold import TSNE

try:
    import umap
    HAS_UMAP = True
except Exception:
    HAS_UMAP = False

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", DEVICE, "| UMAP:", HAS_UMAP)

TARGET_SR = 22050

ds = load_dataset("jamendolyrics/jamendolyrics", split="test")
print(ds)
print("Columns:", ds.column_names)

# IMPORTANT: new datasets versions only accept sampling_rate
ds = ds.cast_column("audio", Audio(sampling_rate=TARGET_SR))

print("Loaded and casted audio.")

def get_audio(example, target_sr=22050) -> Tuple[np.ndarray, int]:
    """
    Version-robust audio extraction:
    - Works if example['audio'] is dict-like
    - Works if it's an AudioDecoder object
    - Falls back to librosa using audio['path'] if array isn't available
    """
    a = example["audio"]

    # Try direct decoding fields (works for dict OR AudioDecoder)
    try:
        y = a["array"]
        sr = a["sampling_rate"]
        return y.astype(np.float32), int(sr)
    except Exception:
        pass

    # Try path-based fallback
    try:
        path = a["path"] if isinstance(a, dict) else getattr(a, "path", None)
        if path is None:
            # sometimes it might be nested
            path = example["audio"]["path"]
        y, sr = librosa.load(path, sr=target_sr, mono=True)
        return y.astype(np.float32), int(sr)
    except Exception as e:
        raise RuntimeError(f"Audio extraction failed for example. Error: {e}")

# quick sanity check
y0, sr0 = get_audio(ds[0], target_sr=TARGET_SR)
print("Audio ok:", y0.shape, "sr:", sr0)
print("Lyrics preview:", (ds[0].get("text","") or "")[:120], "...")

@dataclass
class AudioConfig:
    n_mels: int = 64
    n_fft: int = 1024
    hop_length: int = 256
    clip_seconds: float = 30.0
    max_frames: int = 512

@dataclass
class TrainConfig:
    latent_dim: int = 32
    batch_size: int = 8
    lr: float = 1e-3
    epochs: int = 25
    beta: float = 1.0

acfg = AudioConfig()
tcfg = TrainConfig()
acfg, tcfg

def fix_length_audio(y: np.ndarray, sr: int, clip_seconds: float) -> np.ndarray:
    target_len = int(sr * clip_seconds)
    if len(y) < target_len:
        y = np.pad(y, (0, target_len - len(y)))
    else:
        y = y[:target_len]
    return y

def logmel_spectrogram(y: np.ndarray, sr: int, acfg: AudioConfig) -> np.ndarray:
    y = fix_length_audio(y, sr, acfg.clip_seconds)

    S = librosa.feature.melspectrogram(
        y=y, sr=sr,
        n_fft=acfg.n_fft,
        hop_length=acfg.hop_length,
        n_mels=acfg.n_mels,
        power=2.0
    )  # (n_mels, T)

    S = np.log1p(S).astype(np.float32)

    # pad/crop frames
    T = S.shape[1]
    if T < acfg.max_frames:
        S = np.pad(S, ((0,0),(0, acfg.max_frames - T)))
    else:
        S = S[:, :acfg.max_frames]
    return S

audio_specs = []
texts = []
genre_labels = []
lang_labels = []

for i in range(len(ds)):
    y, sr = get_audio(ds[i], target_sr=TARGET_SR)
    S = logmel_spectrogram(y, sr, acfg)  # (n_mels, max_frames)
    audio_specs.append(S)

    texts.append(ds[i].get("text", "") or "")
    genre_labels.append(ds[i].get("genre", None))
    lang_labels.append(ds[i].get("language", None))

X_audio = np.stack(audio_specs, axis=0)   # (N, n_mels, T)
X_audio = X_audio[:, None, :, :]          # (N, 1, n_mels, T)

print("Audio tensor:", X_audio.shape)
print("Example genre:", genre_labels[0], "| Example language:", lang_labels[0])
print("Lyrics chars:", len(texts[0]))

mean = X_audio.mean()
std = X_audio.std() + 1e-8
X_audio_n = ((X_audio - mean) / std).astype(np.float32)
print("Normalized:", X_audio_n.shape)

from transformers import AutoTokenizer, AutoModel

LYRICS_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

tokenizer = AutoTokenizer.from_pretrained(LYRICS_MODEL)
text_model = AutoModel.from_pretrained(LYRICS_MODEL).to(DEVICE)
text_model.eval()

@torch.no_grad()
def mean_pooling(last_hidden_state, attention_mask):
    mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
    summed = torch.sum(last_hidden_state * mask, dim=1)
    counts = torch.clamp(mask.sum(dim=1), min=1e-9)
    return summed / counts

@torch.no_grad()
def embed_texts(texts: List[str], batch_size: int = 16) -> np.ndarray:
    embs = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        enc = tokenizer(batch, padding=True, truncation=True, max_length=256, return_tensors="pt").to(DEVICE)
        out = text_model(**enc)
        sent = mean_pooling(out.last_hidden_state, enc["attention_mask"])
        embs.append(sent.cpu().numpy())
    return np.vstack(embs)

Z_lyrics = embed_texts(texts, batch_size=16)
print("Lyrics embeddings:", Z_lyrics.shape)

class AudioDataset(Dataset):
    def __init__(self, X: np.ndarray):
        self.X = torch.from_numpy(X)
    def __len__(self):
        return self.X.shape[0]
    def __getitem__(self, idx):
        return self.X[idx]

class ConvVAE(nn.Module):
    def __init__(self, latent_dim: int, n_mels: int, max_frames: int):
        super().__init__()
        self.enc = nn.Sequential(
            nn.Conv2d(1, 32, 4, stride=2, padding=1), nn.ReLU(),   # -> (32, n_mels/2, T/2)
            nn.Conv2d(32, 64, 4, stride=2, padding=1), nn.ReLU(),  # -> (64, /4, /4)
            nn.Conv2d(64, 128, 4, stride=2, padding=1), nn.ReLU(), # -> (128,/8,/8)
        )

        with torch.no_grad():
            dummy = torch.zeros(1, 1, n_mels, max_frames)
            h = self.enc(dummy)
            self.enc_shape = h.shape[1:]  # (C,H,W)
            flat_dim = int(np.prod(self.enc_shape))

        self.fc_mu = nn.Linear(flat_dim, latent_dim)
        self.fc_logvar = nn.Linear(flat_dim, latent_dim)

        self.fc_z = nn.Linear(latent_dim, flat_dim)
        self.dec = nn.Sequential(
            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1), nn.ReLU(),
            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1), nn.ReLU(),
            nn.ConvTranspose2d(32, 1, 4, stride=2, padding=1),
        )

    def encode(self, x):
        h = self.enc(x).view(x.size(0), -1)
        return self.fc_mu(h), self.fc_logvar(h)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        h = self.fc_z(z).view(z.size(0), *self.enc_shape)
        return self.dec(h)

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        x_hat = self.decode(z)
        return x_hat, mu, logvar

def vae_loss(x, x_hat, mu, logvar, beta=1.0):
    recon = F.mse_loss(x_hat, x, reduction="mean")
    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
    return recon + beta * kl, recon.detach(), kl.detach()

train_ds = AudioDataset(X_audio_n)
loader = DataLoader(train_ds, batch_size=tcfg.batch_size, shuffle=True)

vae = ConvVAE(tcfg.latent_dim, acfg.n_mels, acfg.max_frames).to(DEVICE)
opt = torch.optim.Adam(vae.parameters(), lr=tcfg.lr)

hist = {"loss": [], "recon": [], "kl": []}

vae.train()
for epoch in range(1, tcfg.epochs + 1):
    losses, recons, kls = [], [], []
    for xb in loader:
        xb = xb.to(DEVICE)
        opt.zero_grad()
        x_hat, mu, logvar = vae(xb)
        loss, recon, kl = vae_loss(xb, x_hat, mu, logvar, beta=tcfg.beta)
        loss.backward()
        opt.step()
        losses.append(loss.item()); recons.append(recon.item()); kls.append(kl.item())

    hist["loss"].append(float(np.mean(losses)))
    hist["recon"].append(float(np.mean(recons)))
    hist["kl"].append(float(np.mean(kls)))

    if epoch == 1 or epoch % 5 == 0:
        print(f"Epoch {epoch:03d} | loss={hist['loss'][-1]:.4f} recon={hist['recon'][-1]:.4f} kl={hist['kl'][-1]:.4f}")

plt.figure()
plt.plot(hist["loss"], label="total")
plt.plot(hist["recon"], label="recon")
plt.plot(hist["kl"], label="kl")
plt.legend(); plt.title("ConvVAE training"); plt.xlabel("epoch"); plt.ylabel("loss")
plt.show()

vae.eval()
with torch.no_grad():
    X_t = torch.from_numpy(X_audio_n).to(DEVICE)
    mu, logvar = vae.encode(X_t)
    Z_audio = mu.cpu().numpy()

print("Audio latent:", Z_audio.shape)

# Reduce lyrics embeddings -> 32 dims
LYRICS_DIM = 32
pca_lyrics = PCA(n_components=min(LYRICS_DIM, Z_lyrics.shape[1]), random_state=SEED)
Z_lyrics_r = pca_lyrics.fit_transform(Z_lyrics)

# Standardize parts and concatenate
Za = StandardScaler().fit_transform(Z_audio)
Zl = StandardScaler().fit_transform(Z_lyrics_r)

Z_hybrid = np.concatenate([Za, Zl], axis=1)
print("Hybrid:", Z_hybrid.shape)

def label_to_ids(labels: List[Optional[str]]) -> Optional[np.ndarray]:
    if labels is None or all(v is None for v in labels):
        return None
    if any(v is None for v in labels):
        print("[INFO] Some labels missing -> skipping ARI.")
        return None
    uniq = sorted(set(labels))
    m = {u:i for i,u in enumerate(uniq)}
    return np.array([m[v] for v in labels], dtype=int)

y_true = label_to_ids(genre_labels)
print("ARI available?", y_true is not None, "| num classes:", (len(set(y_true)) if y_true is not None else None))

def safe_silhouette(X, labels) -> float:
    uniq = set(labels)
    uniq_no_noise = {u for u in uniq if u != -1}
    if len(uniq_no_noise) < 2:
        return np.nan
    return float(silhouette_score(X, labels))

def safe_db(X, labels) -> float:
    uniq = set(labels)
    uniq_no_noise = {u for u in uniq if u != -1}
    if len(uniq_no_noise) < 2:
        return np.nan
    return float(davies_bouldin_score(X, labels))

def safe_ari(labels, y_true: Optional[np.ndarray]) -> float:
    if y_true is None:
        return np.nan
    return float(adjusted_rand_score(y_true, labels))

def eval_all_clusterers(X_feat: np.ndarray, tag: str, y_true: Optional[np.ndarray],
                        k_list=[3,4,5,6,7,8],
                        dbscan_grid=[(0.4,5),(0.6,5),(0.8,5),(1.0,5)]) -> pd.DataFrame:
    rows = []

    for k in k_list:
        # KMeans
        lab = KMeans(n_clusters=k, n_init=10, random_state=SEED).fit_predict(X_feat)
        rows.append({"feature": tag, "method": f"KMeans(k={k})",
                     "silhouette": safe_silhouette(X_feat, lab),
                     "davies_bouldin": safe_db(X_feat, lab),
                     "ARI_if_labels": safe_ari(lab, y_true)})

        # Agglomerative
        lab = AgglomerativeClustering(n_clusters=k, linkage="ward").fit_predict(X_feat)
        rows.append({"feature": tag, "method": f"Agglomerative(k={k})",
                     "silhouette": safe_silhouette(X_feat, lab),
                     "davies_bouldin": safe_db(X_feat, lab),
                     "ARI_if_labels": safe_ari(lab, y_true)})

    for eps, ms in dbscan_grid:
        lab = DBSCAN(eps=eps, min_samples=ms).fit_predict(X_feat)
        rows.append({"feature": tag, "method": f"DBSCAN(eps={eps},min_samples={ms})",
                     "silhouette": safe_silhouette(X_feat, lab),
                     "davies_bouldin": safe_db(X_feat, lab),
                     "ARI_if_labels": safe_ari(lab, y_true)})

    return pd.DataFrame(rows)

df_audio = eval_all_clusterers(Z_audio, "ConvVAE_audio_latent", y_true)
df_hyb   = eval_all_clusterers(Z_hybrid, "Hybrid(audio_latent + lyrics)", y_true)

df_results = pd.concat([df_audio, df_hyb], ignore_index=True)
display(df_results.sort_values(["feature","silhouette"], ascending=[True,False]))

X_flat = X_audio_n.reshape(X_audio_n.shape[0], -1)

BASE_PCA_DIM = 64
Z_audio_pca = PCA(n_components=min(BASE_PCA_DIM, X_flat.shape[1]), random_state=SEED).fit_transform(X_flat)
Z_audio_pca = StandardScaler().fit_transform(Z_audio_pca)

# Hybrid baseline: [audio_pca, lyrics_pca] then clustering
Z_hybrid_base = np.concatenate([Z_audio_pca, StandardScaler().fit_transform(Z_lyrics_r)], axis=1)

df_base_audio = eval_all_clusterers(Z_audio_pca, "Baseline_PCA(audio_flat)", y_true)
df_base_hyb   = eval_all_clusterers(Z_hybrid_base, "Baseline_Hybrid(audio_PCA + lyrics)", y_true)

df_all = pd.concat([df_results, df_base_audio, df_base_hyb], ignore_index=True)
display(df_all.sort_values("silhouette", ascending=False).head(20))

def plot_metric(df: pd.DataFrame, metric: str, title: str):
    plt.figure(figsize=(10,4))
    x = np.arange(len(df))
    plt.bar(x, df[metric].fillna(0.0))
    plt.xticks(x, df["feature"] + " | " + df["method"], rotation=90)
    plt.title(title)
    plt.tight_layout()
    plt.show()

top = df_all.sort_values("silhouette", ascending=False).head(12)
display(top)

plot_metric(top, "silhouette", "Top configs — Silhouette (higher is better)")
plot_metric(top, "davies_bouldin", "Top configs — Davies–Bouldin (lower is better)")
plot_metric(top, "ARI_if_labels", "Top configs — ARI (higher is better, if labels exist)")

def embed_tsne(X_feat: np.ndarray) -> np.ndarray:
    n = X_feat.shape[0]
    perplex = min(20, max(5, (n - 1)//3))
    return TSNE(n_components=2, random_state=SEED, perplexity=perplex, init="pca", learning_rate="auto").fit_transform(X_feat)

def embed_umap(X_feat: np.ndarray) -> Optional[np.ndarray]:
    if not HAS_UMAP:
        return None
    return umap.UMAP(n_components=2, random_state=SEED, n_neighbors=10, min_dist=0.15).fit_transform(X_feat)

def plot_embedding(E, labels, title):
    plt.figure(figsize=(6,5))
    plt.scatter(E[:,0], E[:,1], c=labels, s=25)
    plt.title(title); plt.xlabel("d1"); plt.ylabel("d2")
    plt.show()

def get_feature_matrix(name: str):
    if name == "ConvVAE_audio_latent":
        return Z_audio
    if name == "Hybrid(audio_latent + lyrics)":
        return Z_hybrid
    if name == "Baseline_PCA(audio_flat)":
        return Z_audio_pca
    if name == "Baseline_Hybrid(audio_PCA + lyrics)":
        return Z_hybrid_base
    raise ValueError("Unknown feature name")

best_rows = df_all.sort_values("silhouette", ascending=False).head(3)
display(best_rows)

for _, r in best_rows.iterrows():
    Xf = get_feature_matrix(r["feature"])
    m = r["method"]

    # build labels from method string
    if m.startswith("KMeans"):
        k = int(m.split("k=")[1].split(")")[0])
        lab = KMeans(n_clusters=k, n_init=10, random_state=SEED).fit_predict(Xf)
    elif m.startswith("Agglomerative"):
        k = int(m.split("k=")[1].split(")")[0])
        lab = AgglomerativeClustering(n_clusters=k).fit_predict(Xf)
    else:
        eps = float(m.split("eps=")[1].split(",")[0])
        ms = int(m.split("min_samples=")[1].split(")")[0])
        lab = DBSCAN(eps=eps, min_samples=ms).fit_predict(Xf)

    Ets = embed_tsne(Xf)
    plot_embedding(Ets, lab, f"{r['feature']} — t-SNE — {m}")

    Eum = embed_umap(Xf)
    if Eum is not None:
        plot_embedding(Eum, lab, f"{r['feature']} — UMAP — {m}")

    # Optional: color by true genre if ARI labels exist
    if y_true is not None:
        plot_embedding(Ets, y_true, "t-SNE — TRUE GENRE (interpretation)")
        if Eum is not None:
            plot_embedding(Eum, y_true, "UMAP — TRUE GENRE (interpretation)")

df_all.to_csv("medium_task_jamendolyrics_results.csv", index=False)
print("Saved: medium_task_jamendolyrics_results.csv")
from google.colab import files
files.download("medium_task_jamendolyrics_results.csv")